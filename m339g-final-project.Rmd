---
title: "The Final Project"
author: "Julian Frank"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
---
<!-- The author of this template is Dr. Gordan Zitkovic.-->
<!-- The code chunk below contains some settings that will  -->
<!-- make your R code look better in the output pdf file.  -->
<!-- If you are curious about what each option below does, -->
<!-- go to https://yihui.org/knitr/options/ -->
<!-- If not, feel free to disregard everything ...  -->
```{r echo=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align="center",
  fig.pos="t",
  strip.white = TRUE
)
```
<!-- ... down to here. -->

---

## Problem #1 **($45$ points)**
Generate a simulated two-class data set with 100 observations and
 two features in which there is a visible but non-linear separation be
tween the two classes. Show that in this setting, a support vector
 machine with a polynomial kernel (with degree greater than 1) or a
 radial kernel will outperform a support vector classifier on the train
ing data. Which technique performs best on the test data? Make
 plots and report training and test error rates in order to back up
 your assertions.
 
 
```{r}
# Generating data separated by a polynomial class decision boundary
set.seed(4)
data <- data.frame(
  x = runif(100, 0),
  y = runif(100, 0)
)
decision_boundary <- 2*(data$x-0.5)^2 + (data$y) - 0.5
data$class <- factor(ifelse(decision_boundary < 0, "A", "B"))
# data

plot(data$x, data$y, pch=19, col = c("red", "green")[data$class])
```
Let's fit the SVMs to the training data

```{r}
library(e1071)

# Split data into training and test sets
set.seed(7) 
train <- 1:70
test <- 71:100

train_data <- data[train, ]
test_data <- data[-train, ]

# Fit the SVM models to the training data
poly_fit <- svm(class ~ ., data = train_data, kernel = "polynomial", degree = 3)
rad_fit <- svm(class ~ ., data = train_data, kernel = "radial")
lin_fit <- svm(class ~ ., data = train_data, kernel = "linear")

```

Now let's see how well they perform on the training data

```{r}
# Predictions on training data
train_pred_poly <- predict(poly_fit, train_data)
train_pred_rad <- predict(rad_fit, train_data)
train_pred_lin <- predict(lin_fit, train_data)

# Training data confusion matrices
print("Confusion matrices:")
table(train_pred_poly, train_data$class)
table(train_pred_rad, train_data$class)
table(train_pred_lin, train_data$class)

# Classification error rates
# Error rate for polynomial SVM
mean(train_pred_poly != train_data$class)
# Error rate for radial SVM
mean(train_pred_rad != train_data$class)
# Error rate for Linear SVM
mean(train_pred_lin != train_data$class)
```
The svm with a radial kernel performs the best on the training dataset with the
lowest classification error rate of 0.02857143.

Now let's see how well they perform on the test data

```{r}
# Predictions on test data
test_pred_poly <- predict(poly_fit, test_data)
test_pred_rad <- predict(rad_fit, test_data)
test_pred_lin <- predict(lin_fit, test_data)

# Test data confusion matrices
table(test_pred_poly, test_data$class)
table(test_pred_rad, test_data$class)
table(test_pred_lin, test_data$class)

# Error rate for polynomial SVM:
mean(test_pred_poly != test_data$class)
# Error rate for radial SVM
mean(test_pred_rad != test_data$class)
# Error rate for linear SVM
mean(test_pred_lin != test_data$class)
```
Again, the SVM with a radial kernel performs the best with the test data,
yielding the lowest classification error rate of 0.06666667

```{r}
# Plot the decision boundaries
# Linear SVM (Support Vector Classifier)
plot(lin_fit, data)

# Polynomial SVM
plot(poly_fit, data)

# Radial SVM
plot(rad_fit, data)
```
Looking at the decision boundaries for all three svm models, it is clear that
the radial kernel SVM is closest to the actual decision boundary. As the actual
decision boundary is non-linear, it outperforms the support vector classifier.


## Problem #2 **($5\times 11=55$ points)**
This problem involves the OJ data set which is part of the ISLR2
 package.
 (a) Create a training set containing a random sample of 800 obser
vations, and a test set containing the remaining observations.

```{r}
library(ISLR2)
library(tree)
data <- ISLR2::OJ
#attach(data)
set.seed(1)

training_indices <- sample(1:nrow(data), 800, replace = FALSE)
train_data <- data[training_indices, ]
test_data <- data[-training_indices, ]
# train_data

```


 (b) Fit a tree to the training data, with Purchase as the response
 and the other variables as predictors. Use the summary() function
 to produce summary statistics about the tree, and describe the
 results obtained. What is the training error rate? How many
 terminal nodes does the tree have?
 
```{r}
tree_fit <- tree(Purchase ~ ., data = train_data)
summary(tree_fit)
```
 
 The recursive binary splitting algorithm ends up using five features
 listed above for the tree. The tree training error rate is 0.1588, and
 the tree has 9 terminal nodes.
 
 (c) Type in the name of the tree object in order to get a detailed
 text output. Pick one of the terminal nodes, and interpret the
 information displayed.
 
```{r}
tree_fit
```
 
 Let's examine the terminal node 8) within the tree: The overall split
 criterion for this class is LoyalCH less than 0.0356415. There are 59
 observations within this branch, and the deviance of the branch is 10.14.
 The overall prediction for data points that fall within this class is MM. 
 
 (d) Create a plot of the tree, and interpret the results.
 
```{r}
plot(tree_fit)
text(tree_fit)
```
 The tree will determine the class of different data points first by 
 considering whether the LoyalCH value for that point is less than 0.5036,
 then moves down the decision path according to whether the split condition
 printed on each split is true or false. We see that of the terminal nodes,
 four are classified with Purchase = MM, and five are classified with
 Purchase = CH.
 
 (e) Predict the response on the test data, and produce a confusion
 matrix comparing the test labels to the predicted test labels.
 What is the test error rate?
 
```{r}
pred <- predict(tree_fit, test_data, type="class")
#pred
table(pred, test_data$Purchase)
error_rate <- mean(pred != test_data$Purchase)
error_rate
```
The test error rate of this tree is 0.1703704.
 
 (f) Apply the cv.tree() function to the training set in order to
 determine the optimal tree size.
 
```{r}
set.seed(134)
cv_tree <- cv.tree(tree_fit, FUN = prune.misclass)
cv_tree
```
 
 (g) Produce a plot with tree size on the x-axis and cross-validated
 classification error rate on the y-axis.
 
```{r}
plot(cv_tree$size, cv_tree$dev, type = "b", xlab = "Size", ylab = "Misclassified", main = "CV_Tree")
min_dev <- which.min(cv_tree$dev)
abline(v = cv_tree$size[min_dev], lty = 2, col = "red")
```
 
 
 (h) Which tree size corresponds to the lowest cross-validated 
 classification error rate?
 
 The tree with size 7 (7 terminal nodes) has the lowest cross-validated
 classification error rate.
 
 (i) Produce a pruned tree corresponding to the optimal tree size
 obtained using cross-validation. If cross-validation does not lead
 to selection of a pruned tree, then create a pruned tree with five
 terminal nodes.
 
```{r}
pruned_tree <- prune.misclass(tree_fit, best = cv_tree$size[min_dev])
plot(pruned_tree)
text(pruned_tree, pretty=0)
```
 
 
 (j) Compare the training error rates between the pruned and un
pruned trees. Which is higher?

```{r}
summary(pruned_tree)
summary(tree_fit)
```
The training error rate for the pruned tree is 0.1625, which is higher than
the the training error of the unpruned tree.

 (k) Compare the test error rates between the pruned and unpruned
 trees. Which is higher?
 
```{r}
pred_pruned <- predict(pruned_tree, test_data, type = "class")
table(pred_pruned, test_data$Purchase)
table(pred, test_data$Purchase)
mean(pred != test_data$Purchase)
mean(pred_pruned != test_data$Purchase)
```
 The test error rate for the unpruned tree is 0.1703704, which is higher
 than the test error rate for the pruned tree.


